{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Unity ML-Agents 기초 : 구조 & 기본 기능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 유니버스 : 유니티ML\n",
    "* 발표자 : 김무성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 차례 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A. Unity ML-Agents의 구조\n",
    "* B. Unity ML-Agents Toolkit\n",
    "* C. Q-Learning 예제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A. Unity ML-Agents의 구조 [1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 참고자료\n",
    "* [1] Bringing Machine Learning to Unity - https://www.slideshare.net/BillLiu31/bringing-machine-learning-to-unity-by-arthur-juliani"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://image.slidesharecdn.com/bringingmltotheunity-arthurjuliani-180123065041/95/bringing-machine-learning-to-unity-by-arthur-juliani-from-unity-14-638.jpg?cb=1516906769\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://image.slidesharecdn.com/bringingmltotheunity-arthurjuliani-180123065041/95/bringing-machine-learning-to-unity-by-arthur-juliani-from-unity-17-1024.jpg?cb=1516906769\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://image.slidesharecdn.com/bringingmltotheunity-arthurjuliani-180123065041/95/bringing-machine-learning-to-unity-by-arthur-juliani-from-unity-21-1024.jpg?cb=1516906769\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://image.slidesharecdn.com/bringingmltotheunity-arthurjuliani-180123065041/95/bringing-machine-learning-to-unity-by-arthur-juliani-from-unity-25-1024.jpg?cb=1516906769\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B. Unity ML-Agents Toolkit [2]\n",
    "## Environment Basics\n",
    "This notebook contains a walkthrough of the basic functions of the Python API for the Unity ML-Agents toolkit. For instructions on building a Unity environment, see [here](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Getting-Started-with-Balance-Ball.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Set environment parameters\n",
    "\n",
    "Be sure to set `env_name` to the name of the Unity environment file you want to launch. Ensure that the environment build is in the `python/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"3DBall\"  # Name of the Unity environment binary to launch\n",
    "train_mode = True  # Whether to run the environment in training or inference mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load dependencies\n",
    "\n",
    "The following loads the necessary dependencies and checks the Python version (at runtime). ML-Agents Toolkit (v0.3 onwards) requires Python 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version:\n",
      "3.6.6 |Anaconda, Inc.| (default, Jun 28 2018, 11:07:29) \n",
      "[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "from unityagents import UnityEnvironment\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Python version:\")\n",
    "print(sys.version)\n",
    "\n",
    "# check Python version\n",
    "if (sys.version_info[0] < 3):\n",
    "    raise Exception(\"ERROR: ML-Agents Toolkit (v0.3 onwards) requires Python 3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Start the environment\n",
    "`UnityEnvironment` launches and begins communication with the environment when instantiated.\n",
    "\n",
    "Environments contain _brains_ which are responsible for deciding the actions of their associated _agents_. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Ball3DAcademy' started successfully!\n",
      "Unity Academy name: Ball3DAcademy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: Ball3DBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unity Academy name: Ball3DAcademy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: Ball3DBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=env_name)\n",
    "\n",
    "# Examine environment parameters\n",
    "print(str(env))\n",
    "\n",
    "# Set the default brain to work with\n",
    "default_brain = env.brain_names[0]\n",
    "brain = env.brains[default_brain]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Examine the observation and state spaces\n",
    "We can reset the environment to be provided with an initial set of observations and states for all the agents within the environment. In ML-Agents, _states_ refer to a vector of variables corresponding to relevant aspects of the environment for an agent. Likewise, _observations_ refer to a set of relevant pixel-wise visuals for an agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent state looks like: \n",
      "[-0.01467304 -0.01468306 -0.52082062  4.         -0.79952097  0.\n",
      "  0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# Reset the environment\n",
    "env_info = env.reset(train_mode=train_mode)[default_brain]\n",
    "\n",
    "# Examine the state space for the default brain\n",
    "print(\"Agent state looks like: \\n{}\".format(env_info.vector_observations[0]))\n",
    "\n",
    "# Examine the observation space for the default brain\n",
    "for observation in env_info.visual_observations:\n",
    "    print(\"Agent observations look like:\")\n",
    "    if observation.shape[3] == 3:\n",
    "        plt.imshow(observation[0,:,:,:])\n",
    "    else:\n",
    "        plt.imshow(observation[0,:,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Take random actions in the environment\n",
    "Once we restart an environment, we can step the environment forward and provide actions to all of the agents within the environment. Here we simply choose random actions based on the `action_space_type` of the default brain. \n",
    "\n",
    "Once this cell is executed, 10 messages will be printed that detail how much reward will be accumulated for the next 10 episodes. The Unity environment will then pause, waiting for further signals telling it what to do next. Thus, not seeing any animation is expected when running this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward this episode: 2.0000000447034836\n",
      "Total reward this episode: 1.2000000327825546\n",
      "Total reward this episode: 1.4000000357627869\n",
      "Total reward this episode: 1.500000037252903\n",
      "Total reward this episode: 1.500000037252903\n",
      "Total reward this episode: 0.5000000223517418\n",
      "Total reward this episode: 2.200000047683716\n",
      "Total reward this episode: 1.2000000327825546\n",
      "Total reward this episode: 1.0000000298023224\n",
      "Total reward this episode: 0.40000002086162567\n"
     ]
    }
   ],
   "source": [
    "for episode in range(10):\n",
    "    env_info = env.reset(train_mode=train_mode)[default_brain]\n",
    "    done = False\n",
    "    episode_rewards = 0\n",
    "    while not done:\n",
    "        if brain.vector_action_space_type == 'continuous':\n",
    "            env_info = env.step(np.random.randn(len(env_info.agents), \n",
    "                                                brain.vector_action_space_size))[default_brain]\n",
    "        else:\n",
    "            env_info = env.step(np.random.randint(0, brain.vector_action_space_size, \n",
    "                                                  size=(len(env_info.agents))))[default_brain]\n",
    "        episode_rewards += env_info.rewards[0]\n",
    "        done = env_info.local_done[0]\n",
    "    print(\"Total reward this episode: {}\".format(episode_rewards))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Close the environment when finished\n",
    "When we are finished using an environment, we can close it with the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C. Q-Learning 예제 [3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Basics\n",
    "* 이번 예제는 유니티의 ml-agents 기본 환경으로 있는 GridWorld를 바이너리 형태로 구성해야 한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from unityagents import UnityEnvironment\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Set environment parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"GridWorld\" # Name of the Unity environment binary to launch\n",
    "train_mode = True # Whether to run the environment in training or inference mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Start the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'GridAcademy' started successfully!\n",
      "Unity Academy name: GridAcademy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tnumObstacles -> 1.0\n",
      "\t\tgridSize -> 5.0\n",
      "\t\tnumGoals -> 1.0\n",
      "Unity brain name: GridWorldBrain\n",
      "        Number of Visual Observations (per agent): 1\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 0\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unity Academy name: GridAcademy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tnumObstacles -> 1.0\n",
      "\t\tgridSize -> 5.0\n",
      "\t\tnumGoals -> 1.0\n",
      "Unity brain name: GridWorldBrain\n",
      "        Number of Visual Observations (per agent): 1\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 0\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=env_name)\n",
    "\n",
    "# Examine environment parameters\n",
    "print(str(env))\n",
    "\n",
    "# Set the default brain to work with\n",
    "default_brain = env.brain_names[0]\n",
    "brain = env.brains[default_brain]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Examine the observation and state spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent state looks like: \n",
      "[]\n",
      "Agent observations look like:\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJztnXmQZHdx579Zr66uPqaP6blHM5KQQUI2Ep5gYTEbXh0LizHgCLMLQYDDsNYagy1ABEjmtsES3rUMOFiwAGMwlzCgRaF1gAkZh+09BEISWCeaQaOZnqNneqbPqq7z5f6Rv3r5K/Xrrurp6p6uefmJ6Ohf/979Xv9e5svMXyYxMwzDSBap830ChmFsPDbwDSOB2MA3jARiA98wEogNfMNIIDbwDSOB2MA3jASypoFPRC8joieI6CAR3dytkzIMY32hcw3gIaIAwM8AXA9gAsCPALyOmR/t3ukZhrEepNew7QsAHGTmnwMAEX0dwKsALDvwgyDgTCazhkMahrEStVoNjUaD2q23loG/G8BR7+8JAP9mpQ0ymQz2XrR/DYc0DGMljh453NF6axn4cW+VJd8NRHQDgBsAIJ1ey+EMw+gWazHuTQDY6/29B8DxZ67EzHcw8wFmPhAEwRoOZxhGt1jLwP8RgMuI6GIiygJ4LYC7u3NahmGsJ+esezNznYjeBuB7AAIAf8XMj3TtzAzDWDfW9NHNzH8H4O+6dC6GYWwQFrlnGAnEBr5hJBAb+IaRQGzgG0YCsYFvGAnEBr5hJBAb+IaRQGzgG0YCsYFvGAnEBr5hJBAb+IaRQGzgG0YCsYFvGAnEBr5hJBDLhWVEELXN0bhqrAz75sQkvmEkEBv4hpFA2g58IvorIjpFRA97faNE9H0ietL9Hlnf0zQMo5t0IvH/GsDLntF3M4B7mfkyAPe6vw3D6BHaDnxm/icAZ5/R/SoAX3TtLwJ4dZfPyzCMdeRcv/G3M/MJAHC/t3XvlAzDWG/W3Z1nlXQMY/NxriNxkoh2MvMJItoJ4NRyKzLzHQDuAIB8Pm9O3U2M+dyTw7mq+ncD+C3X/i0A3+nO6RiGsRF04s77GoD/C+DZRDRBRG8GcBuA64noSQDXu78Nw+gR2qr6zPy6ZRZd2+VzMQxjg7DIPcNIIGZmNyLWw7i3HhN/jLVjEt8wEogNfMNIIDbwDSOB2MA3jARiA98wEogNfMNIIDbwDSOBmB8/Dgq7uruemfyyDi73XrhyQnC+T2HDMYlvGAnEBr5hJBAb+IaRQGzgG0YCsYFvGAnEBr5hJBAb+IaRQDpJvbWXiH5ARI8R0SNEdKPrt2o6htGjdCLx6wBuYubLAbwQwFuJ6ApYNR3D6Fk6qaRzgpkfcO15AI8B2A2rpmMYPcuqQnaJaD+AqwHch2dU0yGi2Go6VlDDMDYfHRv3iGgAwLcAvJ2Z5zrdjpnvYOYDzHwgCJIXE20Ym5GOBj4RZSCD/ivM/G3XPemq6KBdNR3DMDYXnVj1CcDnATzGzLd7i6yajmH0KJ18dL8YwBsA/CsRPeT6/hBSPecbrrLOEQCvWZ9TNAyj23RSSedfsPxMbaumYxg9iEXuGUYCsYFvGAnEBr5hJBCLqImhZ3LkGcY5YhLfMBKIDXzDSCA28A0jgdjAN4wEYgPfMBKIDXzDSCA28A0jgdjAN4wEYgPfMBKIDXzDSCA28A0jgdjAN4wEYgPfMBJIJzn38kT0QyL6iauk82HXfzER3ecq6dxJRNn1P13DMLpBJxK/AuAaZn4egKsAvIyIXgjgYwD+3FXSmQbw5vU7TcMwukknlXSYmRfcnxn3wwCuAfBN12+VdAyjh+g0r37gMuyeAvB9AIcAzDBz3a0yASmrFbftDUR0PxHd32g0unHOhmGskY4GPjM3mPkqAHsAvADA5XGrLbOtVdIxjE3Gqqz6zDwD4B8hVXOHiaiZumsPgOPdPTXDMNaLTqz640Q07Np9AK6DVMz9AYDfdKtZJR3D6CE6Sba5E8AXiSiAvCi+wcz3ENGjAL5ORB8B8CCkzJZh9Bx95VLUrgYVAAAX9LO05pqLc2HUt2fwkqhdmRRT11B2MOoLMQsAKGW0pOQZmojawRbZabWqXvAC7wMA0KwWnu7nXVH7xJYHAQCDg1uivuJCFQDQqMqXdqNDJb6TSjo/hZTGfmb/zyHf+4Zh9BgWuWcYCcTy6huJZyqoR+2xrWMAgNLCbNRXXxT1f/eYqt1zZ87ocldaMlPQ4TTtts8XMlFff2571K6SqOgBqexNZd03xaC6vefKp6N2blHCaYKUlrI8M3ECAPDcK58PAFg42dmQNolvGAnEJL6ReIJto1H7qclJAMC2ATWgjQ0MAQCOHXwq6hsfG9fle2T506cORX2je0W6T81MRX2LxWrU3rVLtIfS3Nmoj3gRAFCtLkR9SKlsvrw/J/ucVi3guquvBAA88MhPAQC18uKy1+ljEt8wEogNfMNIIKbqG4knW1NjWT7MAwCopv71Ys0Z97bv0PVSqrbf/PtvBAD86e3/LeqbmHgSADA6tifqW8xtjdrHJuYBAIVcIerLhLLPsUIl6hvIauzA7Te+DwDwO2+9MeprTB2VcxuS852f12tZCZP4hpFAaCNLQufzed570f4NO965wrBZhElidE7HQDUrkjM1NBT1nZ6R6LuxgsrJT33wXVF7T0b+X2aOaWTeH//pJwEA9bGLo77/c0RdgEP7LgUA5DLqSszNPQ0AuAhq8Hv/771Rj7NL4ugmpnQ/b3vvBwAA6W2ijTx0dArz5VpbsW8S3zASiA18w0ggZtwzEs/Fc2rIm+AyAKAxqL791EC/LJvy/PjDOiGncFj69+26NOr7xjvfDwB49bs/EPVdOrYvajcyEtF34pju87l98snwP97+B1HfLlK//GOnTwIAtm3TCMBd28Rg+MikLKvVlr9OH5P4hpFAbOAbRgIxq34MZtVPFr9yWCfSHM+LQfzpQfWfT/WL/rxvx0DUx4/+NGr/063iv08ffDrq68uMSOMXroz63vQ+Vft/7EJ5t4zkor6v3vpeAEDmiYeivu0jesxDWy8CALzxhrdHfTTUJ41hmVz00OFJLJSrZtU3DGMpJvFjMImfLJ41pVJ3zjWP5zV6rj4qNvDKovrXC2d0osxLhmXCzmffeXPUl5kTI2E63xf1HS+Xo/a7PvZHAIC/+PRfRH3HDj4CAPilPZqwunpqUo/zsU8AAIbGNENPycnuyTkxAk6cnka52kU/vkux/SAR3eP+tko6htGjrEbVvxGSZLOJVdIxjB6lI1WfiPZAquV8FMA7Afw6gNMAdjBznYheBOBDzPzSlfazHqo+0cpaTdz1tb1mCldeblxQ1Btq3INrNpNuAkCVpN0/oEptqqzLc7Myf354vhj1/c1HPwoA2Jf1QmVmNdS2tCCfDcOeWn+qIkbE+SAf9b3r/X8UtY9ulXVPntJPjuHxnQCAdF7iCn722MMoFYtdU/U/DuDdAJojYgxWSccwepa2kXtE9AoAp5j5x0T0q83umFWXraQD4A5AJP45nqdhrBtToyrx+9yImJ9So9qOrcMAgLNnVdKW66oV7t4uUndhUI13v3vrHwMAPvfO39X1Kirxh7fLPk8/8sOoL/NsmYTzlveplJ8pjEXt086MlhpXGXu6KsI0XBRto1bvTFvtJGT3xQBeSUQvB5AHMATRAIaJKO2kvlXSMYweopNqubcw8x5m3g/gtQD+gZlfD6ukYxg9y1om6bwHVknHuAAoe0a5LVvFSDY46PnfTxwBAIyP6+SYZ41fFLUf/BdR139xp6rgizOyz4sv1vn4eEpTdqMsav+4U/kB4GRJDIbDA5rI89ikfj4s5ESNHxlW9b9elG1Ks7Jep2bpVQ18Zv5HSNFMq6RjGD2MhewaRgLp+ZBd8+Mba6URqn++nhKV+Wz5ZNS352LJgV8pesU1T+nnwXMGZPngrPbdftNbAQDj6Zmobyg3F7XnFiTkd+iSK6K+Jx51Ofjzz4r6PnPHt6L2FyvyyUFe9Z2xUfEozM1KyO6ZicdRK5dsko5hGEuxDDxG4rk4pSmuD50Rqbt//96o79gR8VTvGdbqOkFFg9EGZo4BAL7wwY9GfaMDMtunVNIMOidZDYZvu+0bAIA/uVWTdm7bJ5I+P6dTcW/6tV+P2g/f920AwAMP6bTddFqOM8Aiw2c61OBN4htGArGBbxgJxFR9I/HUvAyVhayo2WFJVebtfeI3Lx9Tg991+y+J2je94jcAAKMjXvWdKSlfPd2v4cDv/csvR+0n3O83fUTn43/pI7cDAPKzGgS75wqNF/ivo9cCAO5q6AShQ65MdjktE3tSHXryTeIbRgIxd17sTs2dlyQqJTXU9Q+KxC/kdWrs6cNSB29/Tv/XvnzrB3WbokThbelT6b7gKvK89C2alWd+q1bnyW+XmnrVGY3m65sUSX/Pn92m63klsSs7LwMAPH5Yc/vd+slPAQCKOTEcPnRyCvPdzMBjGMaFgw18w0ggZtwzEk91h1bFKRfF0Dd7UlXsA5f9IgCg9LOfRH1Z1s/B9IBo1pMNjdL7nVvEUJfer5NwahlNknn8rEQIjmb6o77xXRIBeMMfvifq+9QH3xK1G8ekJPYLn3NZ1JdalOw/6ZTT7s2PbxjGctjAN4wE0vNW/fVgL4uX9YxnG21kxWoazmoIZtPIu5jTedjzqZ1Re4vLz7917udR33CmCgA4WdNc7qUhnV9drkrI6MiWatR3ZFqO2RgdifrOZDWUE1VJCXXlohZg3FoXn3MxpT7qyZpanYO0qJ3ZWhD1pUmOWcnqZJOil4cyhJxzoaKhpzmXdZH95JRpbadCvc7NSm1B739fv0zECaDPuTov1zDSr/PxG6npqH3LhyTB9If/5Naob7EiKj6l9TOiHsxrOxRrftGbor97XObuLxbVjz8woJ8Pb3vLZwAAn/3sZ6O+w4cPAwC2bZPnefCph7C4OG9WfcMwltKRcY+IDgOYB9AAUGfmA0Q0CuBOAPsBHAbwn5h5erl99BKHyiLmwpH9UV85IxM06gW9xP4+mdxR6tsT9U0v6gSL41WRGuN9aggaTYsEnsqq1Kzkt0btekokRS1bj/oWtrjSzUO79CQLaiiC0wimJ1WiBFWZAsr9+m5PZXQySiYnfuYsqcQPayLxuUUeXPgxDUPe5JtKVURwpaz3spATn36lrprMQkmn2H75S3cCAIoLqqWNjkuZ7cefUi1s+169r6HT0n7hcp2We+yQGBQDTwmfndP7/6Uv/xkA4MjEk1Ffv8sU1NcvRr5UqrNM1quR+P+ema9i5gPu75sB3OsKatzr/jYMowdYi6r/KkiRDbjfr1776RiGsRF06sdnAH9PRAzgL12u/O3MfAIAmPkEEW1bcQ89RHnPKwEAv3T9a6K+7HaZn724eMZbU9Tx2fyOqGeARqP2uJuTva14JOrbAlEXz9RU7a5n1GhXXxCj3HDgGdhCUccX8nqLywXVB0d/LvOzF76nRqHslKh+5YyuR4Fa6io1UQmrRVVf+xNa/bDBC/pHIMa9MKVWt3RePt+4qvdvYEj97//8v/8fAGDHRfuivqcnRMXfe4mG6X7xzg95x3E5+mtqWHz9b7xbGhUNF87lNPHmzw7Kc87l1WA6MCSye9Hl7A9ZPxFXotOB/2JmPu4G9/eJ6PEOtwMR3QDgBgBIpy1eyDA2Ax2NRGY+7n6fIqK7INl1J4lop5P2OwGcWmbbnqukc6osxrrK9hdHfZVt0lcq6tTMkjMEzWT1rT7JKgkqA/Lmnq2oa2jIvZHnSd086YK6iXIZ94ZvqBQK82J8mm7occa26ESNwhbRMiYr+rZvNCV6yvPspNWQ1zTgpVNpr29laZGKnp5v8Ot9x9C05z4bGRVNi7Ka1jpkcbE2anp/tm1VLe+0c/dlPUk8Mi73vUbHor7csB5noXIIAJDP6rTb41OiJYz2qXs48NyBW4fFVZz3JhBVq2JQLBXlvMNGlyL3iKifSP5LiagfwH8A8DCAuyGFNAArqGEYPUUnEn87gLvc9Nc0gK8y83eJ6EcAvkFEbwZwBMBrVtiHYRibiLYD3xXOeF5M/xkA167HSZ13qqIITZdUpZ05JsaTbFp9tXkXurd1SFWvxaJuU58Rv+yAZ2ALnDqeYvW3pkLP91p3pZJLquqns2IAOjnvqeKkqZ77Q5do0TPUjbKo9Q2vvmndU/DI+e+DrB+a15lhyKe5x56ug5z2PpFcBKIX3gCuyb0u17Vzet5T653aPzunvv+Ge+blshoJZ6uHonZ2UP43BvNqvNu9X54VFdXwO3tWPzmGsnJMbuhzrLnIy3RKPgcJnYXS9P4HmmEYq8YGvmEkEPOvxTEgVvgdI6rA9pGo4ENZnfRSn5LJN1nPodHvTUrJVkVF3J1WnzxV5FPhTKhhog1PbW8siqpWKamKOOIquZzx3KFBWi38W8ZF1UwXtK8vLaG/i6SqYt1T+xuurnrg1VNPn5MY6P2Q3kKfTqwqldyz8Izjo/3iNUkHamGfOu1Z+HeLuj63qGG8dff/UhjWz8BsTsO5QeLHP/T00ahrek4OOpr1LPlbdT7/wozEg4ShPkd2XpvIVd4mFV0Tk/iGkUBM4scx9wgAgMoHo6405C0cnp6I+h78jlQGr0xPRX2ZrL7Vh5w0fGpat2ka1ea9aL/KgEZvsZPQnNN38i8Pylu8mtXqLnOkj27eZYOZnlPNYjAlEsmbiQsEapyiQPafIQ3XC8MqkkhY02cRhnIPG54mFGZleSpQLS2b1eXlRbmXee+Z1gK5l9WqanOVkj7n/qw80z4vBqQvJ5GZNW/a8/S0GnmHR+Vh+glmm9PqKzUxSjIsA49hGMtgA98wEoip+jFkBkVNzqZUTcuyqFcjGfWVD5fFqJcqeRlTaqqmZctiJBwJNWQ3dEUOz3ilmc8WVUWfrYhxr+pVYBnKi/q/davu+2xZPykKoah37E3CQSDH8Soqg1NeyK67Hj8DU+qcAqp7X3aUK3pfBobEUFdaVKNoseSWV/Vat4yqAe74acnY1D+inwLVulPBvRgNZp2MBZbnT6w++7L70vINyP3D+vk170K//TkvmYw887o7Ne7MtncBPDXDMFaNSfwY3PwWNFIqlWvuzVwPvMw5LmPNeFbfn7m6Svc+Es0h8Ca/1KvySg4Dfav7799RN7HnjBcLx4E8Jj8yrwadDLSwKOcRZNSVSO7R1kOVXL50bxqImP2YuxiR71cV4qWuu9535gGjO/T5TE3LRJlMVo1qgwV59nNnVbM7u6Du1p2Xyn07M3s46quzm7jTr3eIU6rZzS2IZteXVy0t67I7haTr1b12GIg24ptgq81H5kYydxhDaRLfMBKIDXzDSCCm6scRiuGmzho1tUhimCkGntEnEANbvxeZV/fVeqdOV70JHxWnAi54iS8rnk8+h6YK7vnXecCdj2b34ZS+s9Wgs/J73DfehZGS7k3ciX6rekreNivZ/kJvP9xj8uTk9D9H7TqLCr99h17Dpz/5cQDA1iHNgDRb1BiP7LBE4YXeZCwOJIHqmRmdNBPkNZdDJi+feqePa9TnV+/6sJxDVbdJkVb0SaVf0PZabnj5bW3XAUziG0YisYFvGAnEVP0YMi4RJjW8KihuUsyiN6Gm6CZtLGRVNUt7SdEDlvdqtqG3uey8A0WvWKI/HT/nLO/pujpkM3VZN03qu6/n9DwoJiVWU12nFkt8zHJgyXKKsd6vhtALKQ1WWG+zMDSqdvKyc7acOqv3YGybXE/BrzAUatLVckM8AdmCfp6VazI3P8jp/0NOv+4wOy9qf8P7gCoMyadcLqPHLpY0HHyhrEU7lyP0/j9XwiS+YSSQTivpDAP4HIArITaeNwF4AhdoJZ3+mryFc3V9L6ZS4m9lqN+VXVhczY+O8/yoTTNf1TPEVTjtttG+wMt8kwlF5OQbKivzbsJIzgvDq3iTRJq+dt94l4okuq4XeJK86dNPeRKHYrzy/vI4PaB5D3rNoOdz+qQ+s5Exke7DXpDd1Al5JvVBfSbDQ5dE7Vk3Qaa+qM9xy6BMqJryJnWlsqoJ7RiXUtfFtN7V0qxoDGe9KdnZ3JbY9nKkOtSxOn1anwDwXWZ+DiQN12OwSjqG0bN0kmV3CMC/A/B5AGDmKjPPwCrpGEbP0omqfwmA0wC+QETPA/BjADfiAq6kk3VhrllvfnozOSb5Ya9Omc+GGkqb8drN92roqV+By6vvVzzxt+9z5ZkbXpaVXEOWZ9Ma5pvyFO8UN9V6VVmbansqxqAHIAq/DbyuVLRNu9k6K8uLXlP7Lxq/JmqXypIQ89QRzYf/hv/8fgDAwoze/1HNkYliXXzxgWe8W3DhHl44Bv7XvZ+J2k8/6XLoD+6P+q57yVsAAPt2aVh4Oq0h4DPT7Yfr1MHOEqZ28oTSAJ4P4NPMfDWAIlah1hPRDUR0PxHd32j0dC5Ww7hg6ETiTwCYYOb73N/fhAz8C7aSTsNF59W9iTRhyolG9g1x8iIr1FUzyDW8yTfNqa/QyTMppzEEaX0J+lpCmuQNH3gusWbK56p3Pr7LrSnxW6W7Ow4v1QL8tu/OoxiDILV5Ys2IPfazwvSYxD92WK+34Z7ZJZc+N+orz8rynWMaOZkifWalqis53qcu2kLa5cIrqAvw1ElNv73rIqmWs3BG79X+3c8CAMxPes+W9PkVWMuxL0cqfKjtOkAHEp+ZTwI4SkTPdl3XAngUVknHMHqWTgN4fh/AV4goC+DnAH4b8tKwSjqG0YN0WjTzIQAHYhZdkJV0Ss5HXvJ85c0kLfnQ97+7vrrqwwM1T00Om9VslKZK7KvYqdDbJpC164GuUXaRXMXsUlUeUGMcxRj80NLnTSKJic5rb9S7MNm+Q1X4ckW+WCvls1FfqSifWJTTGI7FRTW6NQLJocA1r5hlRSLoqKDz+rfv1uE2PS8Fp1Osqb2Pu9La4/1XR32ZlH4+pBb9HA7xdJiAp8c+xgzD6AoWqx/DonuxL3p3pynxw7r/rpT3a7bhud48b0pTI6h579dqM9rPk+jgpVNsq97i5nmUMrpef92X2M0Ye//MYmLx/Ww61NQSuiPlwx6WIfPzGl1XGBJj3FzxsPb1y/Ts/j7//mkz5TIfzZe9eoc50dxqoQazzjopDwB9Q2fdtuoDHJaZvKjNa7x9zZuzMZSOtZ+3QGivFQAm8Q0jkdjAN4wEYqp+HDnxpwae2rSXpMbZLGsGnmf93kcBAH716sy4VlOZLokBiGOz06gK56voTa/vkGeIO+yaOyo6eWNX5YdRO3Dptaf8ktd0qeynokaobayPu+qMV6EXuldyqZoXvc8QPw6sGUxYyeg+m5mA/AlNfRVtl88tZ/eypFLdl1V9/Vq+mmuitvcFz9a+hhjYZub94eK1XYLVbFpVdHaZfBqq/WO09BLdZF7+t/yy6qNZMTIWs55Kn9YYkFrNq723DNyhec8kvmEkEBv4hpFATNWPoyHqfGZeLbKZPrG+Zn1V01n652uqEOc9y27dqfqU9ubwx7xrW3z6TjOmuLn1Xt/WYZ2bnXZpY3L9aiEOXFmWWllDS4ukKnqt4j5ZvGwvnO1MDrT4NeJS8fdaOEDb8jNxmQhi+jzvTLPCjR/BPTioz6xcls+CXEZn8czNSkhvWNdJOrns+gxRk/iGkUBM4sewo1/ewqPzGr0VVMTsRn61mpy8mbMlNfhlGl674qbT5lUSNzPWtEh03//u2qk2E2XOzB+N2n0NlzHIWzHntJFMTqcE51L6uNMu2izlleOeD0UKLSewU7xUTiw1VQLUsl6vzsiMuwud5yKcn5N7me9XiX7iuKbK7ivI/1HFy9qzbdsuAMDkCdUa5+e90udZjQxcKybxDSOB2MA3jARiqn4MZ38iqQcemFV/arkiVhoOvFLTzVBNr6Rytk99rQ2nAC94y5vEqfeAqswpDpf0+VS36UYjefnkmDt+JOrL58X3TDU9dtXzz4cuwSc3dD/N/J6+rSv0Q1Mjw6NXfce1U97kpZZro06njZw/Wj6lYr9zwpUWard3X0aHxwAAc0XN5HPTO94TtYuuNPrMtJcOOxRVPpfRCk7jXqqf8kJn2XU6wSS+YSQQk/gx7BsUKZUpTkZ9dWfAa3gSLEyJiMx67rygrAaYdFpu70DGi6hrSkhPELZOp3V9nhhSg59y8IRG8RWddM/6xr2UGNXSXrZl9ibpNKg5iUTPnV3WmLCNkKY4Y2RL3+aX8qujeXG+cS8Vs1z7FkuudHlKXXOnJtU9nM/n3XL9f9m1S6ItZ6dVSysVvbyPXZTTJvENI4HYwDeMBNJW1Xe59u70ui4B8AEAX8IFWklnoCAqXaOu/tQBlyeUA71lTY224OUQZWikXAOisoW1pUahVEuabqVZ7WY5P3+TsQGNAsu7HQRb1LBYaGbl8bb1UwAgK98AFOi7v+iq+LQeLsaQx74hr9lHMVv0hnEvXv4trVTU0tcmOjGbdTUSF0te35C3eznm6LDWZ5w8KXEjxHlvG89339hA4x4zP8HMVzHzVQB+GUAJwF2wSjqG0bOsVtW/FsAhZn4aVknHMHqW1Vr1Xwvga659wVbSma+Jj7VaVYtqJutCL0M1kzeaOfIzehtrXs3r5va5nIb5NolLkwWoVtkaArs0VDRb8Y7pJuRk0tpXqTs/fU2voeY/7UzQcg0AwG3qLcZNILogJum0EPdpEqPqU1zby7vgVPl8dtBbzwtfdp9IoVcctfmost7/U4rUIxTiPPjxXWrtVwL429UcwCrpGMbmYzUS/z8CeICZm87tC7aSTlAQw4x/smFOJH7dk+j1hrzpC95ElxZBQPI2r6c8P72TGtwi8ZWmjaylUk7MOW4hlRQVJ4oH+1SzCF2SxrqXYcfbBKGz9DVafNNL/dGt/vnUkuVxErJ16nHnE1vOFy2Tj/gZvwHoNfiCK2Ybj+KcaI07dmrk3YkTOrGqv180gdk5NSDv3n4RAGBuTifmNKpeJaQu+uBWs6vXQdV8wCrpGEbP0tHAJ6ICgOsBfNvrvg3A9UT0pFt2W/dPzzCM9YCYN077zufzvPei/Rt2vHOlkm5a2JaqqbGKa8x6Qpz/fuk2sSGwWNrnk617eQFijH9x59buU1wMAAAHOUlEQVSImXxTC5b2hVjqp/fbmYaXacbtNO31Bd75LuQ6y/N+Xkl5pc0jC6evyqefsWwZfOMdNQ1xS+sftO6/3XG85eSXYI/n6JHDKJfLbYMnLHLPMBKITdKJIRVmll1GLeI3RtLGSP84iZzy3upxr+e4aL2WKLxwGddS88zcTltmnMZIfH86bVO4By1SyJP44dLsQc19hr1lz2ulnSRv4kv0dmmsm/etZbW4+7qyoXS9bqZJfMNIIDbwDSOBmKofQ7aejemNM/gFrcsAxL1LqY26FpfEstWgt1QFJHjVbJpn4dfhjOnz1fY4rTIIl6qf8ZOFvJwE0cLQ6+steUIx97+VDtXtlv2kYvo6Jf543TTD99YTMgyjK9jAN4wEYqp+DFrv3leuYt6Ry/rvn4mnOsctbklVFfcuXtrXiHlyfqhsvFU/JhQ39jOjc3nQPPWWWRhBr5n1u6TqL/dZFbtCtOIy7eYmZtU3DKNLmMSPxWWvaVczremfb+fbj2Xld25cjT1/m0bMjI046d56Nkslfkuq7Gcsa+1V/AjASOIvk5I70wvCP046x0ra1VxMXI2hNsRl+mlpd2+4msQ3jARiA98wEoip+jE0gmpMb0xu9VjVzCeuf6mvvMXv7Q7DsYYir6R1jKrfMrM+ZnlrBRz5HXgHisuR37L/5qreuTVV/MbSS+gh/JDdFVT8doa2ljgJt26n4cAtx/Ez7fjx0abqG4axBkzix1BJL51Kqoa+mIw1bSRBq/SWbUJ/G09ScGxraU63dIxBatkoPUcQU+GmdeLP0kk4nbqTGjFaAADkLqhsa8vpMs0LXjmCs7UvTouIq9Fn7jzDMLqEDXzDSCAdqfpE9A4A/wWig/wrgN8GsBPA1wGMAngAwBuYOc4q1nNogso4NcuvgLNy2mVd04+oaxp9vIo83vZhzDHVp+8l4IxRoVt8/1HSzg7njSM++08YZ7DylzcjBFt8+z1m3mtzjfG0U+u7RMsnYfd22/aMiWg3gD8AcICZr4SYQF8L4GMA/txV0pkG8ObunZZhGOtJp6+qNIA+IkoDKAA4AeAaAN90y62SjmH0EG1VfWY+RkT/HcARAIsA/h7AjwHMMHPT4TgBYPe6neUGk6ttNmfHuaifK9NUzWtp/WY4l7SYzZDcngjNXQZOtUti2UyCeQ7/Fy3putq5OGKOw0urMHWDTlT9EUidvIsB7ALQDymu8Uxiv0Csko5hbD46UfWvA/AUM59m5hokt/6/BTDsVH8A2APgeNzGzHwHMx9g5gNBsJooJsMw1otOBv4RAC8kogJJsfNrATwK4AcAftOtY5V0DKOHaDvwmfk+iBHvAYgrLwWphfceAO8kooMAxgB8fh3P0zCMLmKVdAzjAsIq6RiGsSw28A0jgdjAN4wEYgPfMBLIhhr3iOg0gCKAqQ076PqzFXY9m5UL6VqAzq5nHzOPt9vRhg58ACCi+5n5wIYedB2x69m8XEjXAnT3ekzVN4wEYgPfMBLI+Rj4d5yHY64ndj2blwvpWoAuXs+Gf+MbhnH+MVXfMBLIhg58InoZET1BRAeJ6OaNPPZaIaK9RPQDInqMiB4hohtd/ygRfZ+InnS/R873ua4GIgqI6EEiusf9fTER3eeu504iyp7vc+wUIhomom8S0ePuOb2ol58PEb3D/a89TERfI6J8t57Phg18IgoAfAqSxOMKAK8jois26vhdoA7gJma+HMALAbzVnf/NAO51uQfvdX/3EjcCeMz7u5dzKX4CwHeZ+TkAnge5rp58Puue65KZN+QHwIsAfM/7+xYAt2zU8dfher4D4HoATwDY6fp2AnjifJ/bKq5hD2QwXAPgHkhu3ikA6bhntpl/AAwBeArObuX19+TzgaSyOwrJYp12z+el3Xo+G6nqNy+kSc/m6SOi/QCuBnAfgO3MfAIA3O9t5+/MVs3HAbwbmqxvDL2bS/ESAKcBfMF9unyOiPrRo8+HmY8BaOa6PAFgFl3MdbmRAz9ujnDPuRSIaADAtwC8nZnnzvf5nCtE9AoAp5j5x353zKq98ozSAJ4P4NPMfDUkNLwn1Po41prrsh0bOfAnAOz1/l42T99mhYgykEH/FWb+tuueJKKdbvlOAKfO1/mtkhcDeCURHYYURrkGogF0lEtxEzIBYIIlYxQgWaOej959PmvKddmOjRz4PwJwmbNKZiGGirs38PhrwuUb/DyAx5j5dm/R3ZCcg0AP5R5k5luYeQ8z74c8i39g5tejR3MpMvNJAEeJ6Nmuq5kbsiefD9Y71+UGGyxeDuBnAA4BeO/5NqCs8tx/BaJW/RTAQ+7n5ZDv4nsBPOl+j57vcz2Ha/tVAPe49iUAfgjgIIC/BZA73+e3iuu4CsD97hn9TwAjvfx8AHwYwOMAHgbwNwBy3Xo+FrlnGAnEIvcMI4HYwDeMBGID3zASiA18w0ggNvANI4HYwDeMBGID3zASiA18w0gg/x+/WPQtfbQwFAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reset the environment\n",
    "env_info = env.reset(train_mode=train_mode)[default_brain]\n",
    "\n",
    "# Examine the state space for the default brain\n",
    "print(\"Agent state looks like: \\n{}\".format(env_info.vector_observations[0]))\n",
    "\n",
    "# Examine the observation space for the default brain\n",
    "for observation in env_info.visual_observations:\n",
    "    print(\"Agent observations look like:\")\n",
    "    if observation.shape[3] == 3:\n",
    "        plt.imshow(observation[0,:,:,:])\n",
    "    else:\n",
    "        plt.imshow(observation[0,:,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Training & Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 2206 / Episode: 100 / Progress: Training / Epsilon: 0.9779500000001004 / Avg Score: -0.22303029827096246\n",
      "Step: 4590 / Episode: 200 / Progress: Training / Epsilon: 0.9541100000002088 / Avg Score: -0.14879999488592147\n",
      "Step: 7325 / Episode: 300 / Progress: Training / Epsilon: 0.9267600000003333 / Avg Score: -0.3036999941058457\n",
      "Step: 10283 / Episode: 400 / Progress: Training / Epsilon: 0.8971800000004679 / Avg Score: -0.24599999360740185\n",
      "Step: 13060 / Episode: 500 / Progress: Training / Epsilon: 0.8694100000005943 / Avg Score: -0.1383999940007925\n",
      "Step: 16202 / Episode: 600 / Progress: Training / Epsilon: 0.8379900000007373 / Avg Score: -0.25489999318495393\n",
      "Step: 19557 / Episode: 700 / Progress: Training / Epsilon: 0.80444000000089 / Avg Score: -0.2557999927178025\n",
      "Step: 22379 / Episode: 800 / Progress: Training / Epsilon: 0.7762200000010184 / Avg Score: -0.22289999390020968\n",
      "Step: 25343 / Episode: 900 / Progress: Training / Epsilon: 0.7465800000011533 / Avg Score: -0.27709999358281495\n",
      "Step: 28507 / Episode: 1000 / Progress: Training / Epsilon: 0.7149400000012973 / Avg Score: -0.3871999931335449\n",
      "Step: 32116 / Episode: 1100 / Progress: Training / Epsilon: 0.6788500000014616 / Avg Score: -0.20219999212771655\n",
      "Step: 36075 / Episode: 1200 / Progress: Training / Epsilon: 0.6392600000016417 / Avg Score: -0.3771999913454056\n",
      "Step: 39126 / Episode: 1300 / Progress: Training / Epsilon: 0.6087500000017806 / Avg Score: -0.3758999933861196\n",
      "Step: 43367 / Episode: 1400 / Progress: Training / Epsilon: 0.5663400000019736 / Avg Score: -0.3358999907039106\n",
      "Step: 47545 / Episode: 1500 / Progress: Training / Epsilon: 0.5245600000021637 / Avg Score: -0.2693999908491969\n",
      "Step: 50996 / Episode: 1600 / Progress: Training / Epsilon: 0.49005000000226556 / Avg Score: -0.28719999246299266\n",
      "Step: 55486 / Episode: 1700 / Progress: Training / Epsilon: 0.44515000000222066 / Avg Score: -0.5013999901339412\n",
      "Step: 60391 / Episode: 1800 / Progress: Training / Epsilon: 0.3961000000021716 / Avg Score: -0.5331999891996384\n",
      "Step: 65068 / Episode: 1900 / Progress: Training / Epsilon: 0.34933000000212483 / Avg Score: -0.5307999897003174\n",
      "Step: 69803 / Episode: 2000 / Progress: Training / Epsilon: 0.3019800000020775 / Avg Score: -0.49639998957514764\n",
      "Step: 75648 / Episode: 2100 / Progress: Training / Epsilon: 0.24353000000201902 / Avg Score: -0.6982999870739878\n",
      "Step: 81281 / Episode: 2200 / Progress: Training / Epsilon: 0.18720000000196269 / Avg Score: -0.6675999875366688\n",
      "Step: 87532 / Episode: 2300 / Progress: Training / Epsilon: 0.1246900000019006 / Avg Score: -0.5602999861352146\n",
      "Step: 94234 / Episode: 2400 / Progress: Training / Epsilon: 0.09999000000191018 / Avg Score: -0.4556999851204455\n",
      "Step: 100269 / Episode: 2500 / Progress: Testing / Epsilon: 0 / Avg Score: -0.6497999865934253\n",
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Initial parameters\n",
    "Num_Training = 100000\n",
    "Num_Testing  = 1000 \n",
    "\n",
    "learning_rate = 0.1\n",
    "gamma = 0.9\n",
    "first_epsilon = 1.0\n",
    "final_epsilon = 0.1\n",
    "\n",
    "epsilon = first_epsilon\n",
    "\n",
    "step = 1\n",
    "score = 0 \n",
    "episode = 1\n",
    "\n",
    "# Empty Q table\n",
    "Q_table = {}\n",
    "\n",
    "\n",
    "train_mode = True\n",
    "env_info = env.reset(train_mode=train_mode)[default_brain]\n",
    "state = env_info.vector_observations[0]\n",
    "state = str(state)\n",
    "\n",
    "score_list = []\n",
    "check_show_progress = 0\n",
    "\n",
    "while True:\n",
    "    if step <= Num_Training:\n",
    "        progress = 'Training'\n",
    "        train_mode = True\n",
    "        \n",
    "        # Choose action\n",
    "        if random.random() < epsilon or state not in Q_table.keys() :\n",
    "            # Choose random action\n",
    "            action = np.random.randint(0, brain.vector_action_space_size, size=(len(env_info.agents)))\n",
    "        else:\n",
    "            # Choose greedy action\n",
    "            action = [np.argmax(Q_table[state])]\n",
    "  \n",
    "        env_info = env.step(action)[default_brain]\n",
    "\n",
    "        next_state = env_info.vector_observations[0]\n",
    "        next_state = str(next_state)\n",
    "        reward = env_info.rewards[0]\n",
    "        terminal = env_info.local_done[0]\n",
    "        \n",
    "        # Decrease epsilon while training\n",
    "        if epsilon > final_epsilon:\n",
    "            epsilon -= first_epsilon/Num_Training\n",
    "\n",
    "    elif step <= Num_Training + Num_Testing:\n",
    "        progress = 'Testing'\n",
    "        train_mode = False\n",
    "        \n",
    "        # Choose greedy action\n",
    "        action = [np.argmax(Q_table[state])]\n",
    "\n",
    "        env_info = env.step(action)[default_brain]\n",
    "\n",
    "        next_state = env_info.vector_observations[0]\n",
    "        next_state = str(next_state)\n",
    "        reward = env_info.rewards[0]\n",
    "        terminal = env_info.local_done[0]\n",
    "        \n",
    "        epsilon = 0\n",
    "\n",
    "    else:\n",
    "        # Close the environment when learning is finished!\n",
    "        print('Finished!')\n",
    "        env.close()\n",
    "        break\n",
    "\n",
    "    # Update Q-table!\n",
    "    if state in Q_table.keys() and next_state in Q_table.keys():\n",
    "        if terminal == True:\n",
    "            Q_table[state][action[0]] = (1 - learning_rate) * Q_table[state][action[0]] + learning_rate * (reward)\n",
    "        else:\n",
    "            Q_table[state][action[0]] = (1 - learning_rate) * Q_table[state][action[0]] + learning_rate * (reward + gamma * max(Q_table[next_state]))\n",
    "\n",
    "\n",
    "    # If state or next state is not in Q-table, then add it with zeros\n",
    "    if state not in Q_table.keys():\n",
    "        Q_table[state] = []\n",
    "        for i in range(brain.vector_action_space_size):\n",
    "            Q_table[state].append(0)\n",
    "    elif next_state not in Q_table.keys():\n",
    "        Q_table[next_state] = []\n",
    "        for i in range(brain.vector_action_space_size):\n",
    "            Q_table[next_state].append(0)\t\t\n",
    "\n",
    "    state = next_state\n",
    "    score += env_info.rewards[0]\n",
    "    step += 1\n",
    "\n",
    "       \n",
    "    # If terminal\n",
    "    if terminal == True:\n",
    "        score_list.append(score)\n",
    "        \n",
    "        check_show_progress = 1\n",
    "        episode += 1\n",
    "        score = 0\n",
    "\n",
    "        # If game is finished, initialize the state\n",
    "        env_info = env.reset(train_mode=train_mode)[default_brain]\n",
    "        state = env_info.vector_observations[0]\n",
    "        state = str(state)\n",
    "        \n",
    "    # Show Progress\n",
    "    if episode % 100 == 0 and check_show_progress == 1:\n",
    "        print('Step: ' + str(step) + ' / ' + 'Episode: ' + str(episode) + ' / ' + 'Progress: ' + progress + ' / ' + 'Epsilon: ' + str(epsilon) + ' / ' + 'Avg Score: ' + str(np.mean(score_list)))\n",
    "        score_list = []\n",
    "        check_show_progress = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 참고자료\n",
    "* [1] Bringing Machine Learning to Unity - https://www.slideshare.net/BillLiu31/bringing-machine-learning-to-unity-by-arthur-juliani\n",
    "* [2] Unity ML-Agents 공식 저장소의 Basic.ipynb - https://github.com/Unity-Technologies/ml-agents/blob/master/python/Basics.ipynb\n",
    "* [3] Kyushik Min's Github : Reinforcement Learning algorithms with Unity ML Agent - https://github.com/Kyushik/Unity_ML_Agent"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
