{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Unity ML-Agents 환경 구성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 유니버스 : 유니티ML\n",
    "* 유니버스X싸이그래머X딥리워드 / 김무성    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 차례 \n",
    "* 1. 실습 github\n",
    "* 2. 설치 환경 & 실행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 실습 github"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 저장소 주소 - https://github.com/psygrammer/Universe\n",
    "* 여기서 UnityRL 디렉토리    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 설치 환경 & 실행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 도커를 이용할 경우 [1]\n",
    "    - https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Using-Docker.md\n",
    "    - https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Getting-Started-with-Balance-Ball.md\n",
    "\n",
    "* 아나콘다를 이용할 경우 [2, 3]\n",
    "    - https://wonseokjung.github.io/unity/update/unity-2/\n",
    "    - https://medium.com/@indiecontessa/setting-up-a-python-environment-with-tensorflow-on-macos-for-training-unity-ml-agents-faf19d71201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m3dball.app\u001b[m\u001b[m            \u001b[31mlearn.py\u001b[m\u001b[m              trainer_config.yaml\r\n",
      "\u001b[31mBasics.ipynb\u001b[m\u001b[m          requirements.txt      unity-environment.log\r\n",
      "\u001b[34mcommunicator_objects\u001b[m\u001b[m  \u001b[31msetup.py\u001b[m\u001b[m              \u001b[34munityagents\u001b[m\u001b[m\r\n",
      "\u001b[34mcurricula\u001b[m\u001b[m             \u001b[34mtests\u001b[m\u001b[m                 \u001b[34munitytrainers\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "!ls python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    \n",
      "                    ▄▄▄▓▓▓▓\n",
      "               ╓▓▓▓▓▓▓█▓▓▓▓▓\n",
      "          ,▄▄▄m▀▀▀'  ,▓▓▓▀▓▓▄                           ▓▓▓  ▓▓▌\n",
      "        ▄▓▓▓▀'      ▄▓▓▀  ▓▓▓      ▄▄     ▄▄ ,▄▄ ▄▄▄▄   ,▄▄ ▄▓▓▌▄ ▄▄▄    ,▄▄\n",
      "      ▄▓▓▓▀        ▄▓▓▀   ▐▓▓▌     ▓▓▌   ▐▓▓ ▐▓▓▓▀▀▀▓▓▌ ▓▓▓ ▀▓▓▌▀ ^▓▓▌  ╒▓▓▌\n",
      "    ▄▓▓▓▓▓▄▄▄▄▄▄▄▄▓▓▓      ▓▀      ▓▓▌   ▐▓▓ ▐▓▓    ▓▓▓ ▓▓▓  ▓▓▌   ▐▓▓▄ ▓▓▌\n",
      "    ▀▓▓▓▓▀▀▀▀▀▀▀▀▀▀▓▓▄     ▓▓      ▓▓▌   ▐▓▓ ▐▓▓    ▓▓▓ ▓▓▓  ▓▓▌    ▐▓▓▐▓▓\n",
      "      ^█▓▓▓        ▀▓▓▄   ▐▓▓▌     ▓▓▓▓▄▓▓▓▓ ▐▓▓    ▓▓▓ ▓▓▓  ▓▓▓▄    ▓▓▓▓`\n",
      "        '▀▓▓▓▄      ^▓▓▓  ▓▓▓       └▀▀▀▀ ▀▀ ^▀▀    `▀▀ `▀▀   '▀▀    ▐▓▓▌\n",
      "           ▀▀▀▀▓▄▄▄   ▓▓▓▓▓▓,                                      ▓▓▓▓▀\n",
      "               `▀█▓▓▓▓▓▓▓▓▓▌\n",
      "                    ¬`▀▀▀█▓\n",
      "                    \n",
      "\n",
      "INFO:unityagents:{'--curriculum': 'None',\n",
      " '--docker-target-name': 'Empty',\n",
      " '--help': False,\n",
      " '--keep-checkpoints': '5',\n",
      " '--lesson': '0',\n",
      " '--load': False,\n",
      " '--no-graphics': False,\n",
      " '--run-id': 'ppo',\n",
      " '--save-freq': '50000',\n",
      " '--seed': '-1',\n",
      " '--slow': False,\n",
      " '--train': True,\n",
      " '--worker-id': '0',\n",
      " '<env>': 'python/3dball.app'}\n",
      "CrashReporter: initialized\n",
      "Mono path[0] = '/Users/moodern/work/psygrammer/Universe/UnityRL/notebooks/python/3dball.app/Contents/Resources/Data/Managed'\n",
      "Mono config path = '/Users/moodern/work/psygrammer/Universe/UnityRL/notebooks/python/3dball.app/Contents/MonoBleedingEdge/etc'\n",
      "PlayerConnection initialized from /Users/moodern/work/psygrammer/Universe/UnityRL/notebooks/python/3dball.app/Contents/Resources/Data (debug = 0)\n",
      "PlayerConnection initialized network socket : 0.0.0.0 55479\n",
      "Multi-casting \"[IP] 192.168.100.65 [Port] 55479 [Flags] 2 [Guid] 2383097873 [EditorId] 3106340071 [Version] 1048832 [Id] OSXPlayer(MooSungui-MacBook-Pro.local) [Debug] 0 [PackageName] OSXPlayer\" to [225.0.0.222:54997]...\n",
      "Started listening to [0.0.0.0:55479]\n",
      "PlayerConnection already initialized - listening to [0.0.0.0:55479]\n",
      "INFO:unityagents:\n",
      "'Ball3DAcademy' started successfully!\n",
      "Unity Academy name: Ball3DAcademy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: Ball3DBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n",
      "2018-08-11 03:07:03.156022: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "INFO:unityagents:Hyperparameters for the PPO Trainer of brain Ball3DBrain: \n",
      "\tbatch_size:\t64\n",
      "\tbeta:\t0.001\n",
      "\tbuffer_size:\t12000\n",
      "\tepsilon:\t0.2\n",
      "\tgamma:\t0.995\n",
      "\thidden_units:\t128\n",
      "\tlambd:\t0.99\n",
      "\tlearning_rate:\t0.0003\n",
      "\tmax_steps:\t5.0e4\n",
      "\tnormalize:\tTrue\n",
      "\tnum_epoch:\t3\n",
      "\tnum_layers:\t2\n",
      "\ttime_horizon:\t1000\n",
      "\tsequence_length:\t64\n",
      "\tsummary_freq:\t1000\n",
      "\tuse_recurrent:\tFalse\n",
      "\tgraph_scope:\t\n",
      "\tsummary_path:\t./summaries/ppo\n",
      "\tmemory_size:\t256\n",
      "\tuse_curiosity:\tFalse\n",
      "\tcuriosity_strength:\t0.01\n",
      "\tcuriosity_enc_size:\t128\n",
      "INFO:unityagents: Ball3DBrain: Step: 1000. Mean Reward: 1.201. Std of Reward: 0.676.\n",
      "INFO:unityagents: Ball3DBrain: Step: 2000. Mean Reward: 1.288. Std of Reward: 0.723.\n",
      "INFO:unityagents: Ball3DBrain: Step: 3000. Mean Reward: 1.587. Std of Reward: 0.952.\n",
      "INFO:unityagents: Ball3DBrain: Step: 4000. Mean Reward: 2.177. Std of Reward: 1.336.\n",
      "INFO:unityagents: Ball3DBrain: Step: 5000. Mean Reward: 3.425. Std of Reward: 2.443.\n",
      "INFO:unityagents: Ball3DBrain: Step: 6000. Mean Reward: 4.729. Std of Reward: 3.888.\n",
      "INFO:unityagents: Ball3DBrain: Step: 7000. Mean Reward: 7.969. Std of Reward: 6.696.\n",
      "INFO:unityagents: Ball3DBrain: Step: 8000. Mean Reward: 14.483. Std of Reward: 13.537.\n",
      "INFO:unityagents: Ball3DBrain: Step: 9000. Mean Reward: 27.647. Std of Reward: 29.762.\n",
      "INFO:unityagents: Ball3DBrain: Step: 10000. Mean Reward: 38.493. Std of Reward: 35.740.\n",
      "INFO:unityagents: Ball3DBrain: Step: 11000. Mean Reward: 80.013. Std of Reward: 28.078.\n",
      "INFO:unityagents: Ball3DBrain: Step: 12000. Mean Reward: 58.716. Std of Reward: 39.074.\n",
      "INFO:unityagents: Ball3DBrain: Step: 13000. Mean Reward: 57.945. Std of Reward: 33.991.\n",
      "INFO:unityagents: Ball3DBrain: Step: 14000. Mean Reward: 79.664. Std of Reward: 32.402.\n",
      "INFO:unityagents: Ball3DBrain: Step: 15000. Mean Reward: 89.514. Std of Reward: 22.071.\n",
      "INFO:unityagents: Ball3DBrain: Step: 16000. Mean Reward: 86.057. Std of Reward: 31.568.\n",
      "INFO:unityagents: Ball3DBrain: Step: 17000. Mean Reward: 84.464. Std of Reward: 33.336.\n",
      "INFO:unityagents: Ball3DBrain: Step: 18000. Mean Reward: 85.986. Std of Reward: 34.328.\n",
      "INFO:unityagents: Ball3DBrain: Step: 19000. Mean Reward: 77.673. Std of Reward: 33.510.\n",
      "INFO:unityagents: Ball3DBrain: Step: 20000. Mean Reward: 100.000. Std of Reward: 0.000.\n",
      "INFO:unityagents: Ball3DBrain: Step: 21000. Mean Reward: 94.308. Std of Reward: 19.719.\n",
      "INFO:unityagents: Ball3DBrain: Step: 22000. Mean Reward: 89.857. Std of Reward: 26.732.\n",
      "INFO:unityagents: Ball3DBrain: Step: 23000. Mean Reward: 76.771. Std of Reward: 31.799.\n",
      "INFO:unityagents: Ball3DBrain: Step: 24000. Mean Reward: 78.038. Std of Reward: 32.744.\n",
      "INFO:unityagents: Ball3DBrain: Step: 25000. Mean Reward: 81.850. Std of Reward: 29.789.\n",
      "INFO:unityagents: Ball3DBrain: Step: 26000. Mean Reward: 80.879. Std of Reward: 27.095.\n",
      "INFO:unityagents: Ball3DBrain: Step: 27000. Mean Reward: 80.060. Std of Reward: 34.266.\n",
      "INFO:unityagents: Ball3DBrain: Step: 28000. Mean Reward: 71.059. Std of Reward: 37.278.\n",
      "INFO:unityagents: Ball3DBrain: Step: 29000. Mean Reward: 71.647. Std of Reward: 32.231.\n",
      "INFO:unityagents: Ball3DBrain: Step: 30000. Mean Reward: 84.807. Std of Reward: 27.550.\n",
      "INFO:unityagents: Ball3DBrain: Step: 31000. Mean Reward: 76.413. Std of Reward: 39.548.\n",
      "INFO:unityagents: Ball3DBrain: Step: 32000. Mean Reward: 95.233. Std of Reward: 11.274.\n",
      "INFO:unityagents: Ball3DBrain: Step: 33000. Mean Reward: 81.040. Std of Reward: 34.746.\n",
      "INFO:unityagents: Ball3DBrain: Step: 34000. Mean Reward: 85.907. Std of Reward: 34.520.\n",
      "INFO:unityagents: Ball3DBrain: Step: 35000. Mean Reward: 82.114. Std of Reward: 34.621.\n",
      "INFO:unityagents: Ball3DBrain: Step: 36000. Mean Reward: 80.644. Std of Reward: 29.739.\n",
      "INFO:unityagents: Ball3DBrain: Step: 37000. Mean Reward: 93.892. Std of Reward: 15.096.\n",
      "INFO:unityagents: Ball3DBrain: Step: 38000. Mean Reward: 94.400. Std of Reward: 19.399.\n",
      "INFO:unityagents: Ball3DBrain: Step: 39000. Mean Reward: 65.741. Std of Reward: 40.520.\n",
      "INFO:unityagents: Ball3DBrain: Step: 40000. Mean Reward: 93.983. Std of Reward: 19.955.\n",
      "INFO:unityagents: Ball3DBrain: Step: 41000. Mean Reward: 100.000. Std of Reward: 0.000.\n",
      "INFO:unityagents: Ball3DBrain: Step: 42000. Mean Reward: 81.779. Std of Reward: 34.000.\n",
      "INFO:unityagents: Ball3DBrain: Step: 43000. Mean Reward: 70.984. Std of Reward: 37.576.\n",
      "INFO:unityagents: Ball3DBrain: Step: 44000. Mean Reward: 71.600. Std of Reward: 38.866.\n",
      "INFO:unityagents: Ball3DBrain: Step: 45000. Mean Reward: 81.421. Std of Reward: 35.658.\n",
      "INFO:unityagents: Ball3DBrain: Step: 46000. Mean Reward: 85.385. Std of Reward: 28.985.\n",
      "INFO:unityagents: Ball3DBrain: Step: 47000. Mean Reward: 91.879. Std of Reward: 18.338.\n",
      "INFO:unityagents: Ball3DBrain: Step: 48000. Mean Reward: 100.000. Std of Reward: 0.000.\n",
      "INFO:unityagents: Ball3DBrain: Step: 49000. Mean Reward: 83.500. Std of Reward: 33.515.\n",
      "INFO:unityagents:Saved Model\n",
      "INFO:unityagents: Ball3DBrain: Step: 50000. Mean Reward: 95.000. Std of Reward: 16.583.\n",
      "INFO:unityagents:Saved Model\n",
      "INFO:unityagents:List of nodes to export :\n",
      "INFO:unityagents:\taction\n",
      "INFO:unityagents:\tvalue_estimate\n",
      "INFO:unityagents:\taction_probs\n",
      "INFO:tensorflow:Restoring parameters from ./models/ppo/model-50001.cptk\n",
      "INFO:tensorflow:Froze 16 variables.\n",
      "INFO:tensorflow:Converted 16 variables to const ops.\n"
     ]
    }
   ],
   "source": [
    "!python python/learn.py python/3dball.app --train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3dball_ppo.bytes                     model-50001.cptk.data-00000-of-00001\r\n",
      "checkpoint                           model-50001.cptk.index\r\n",
      "model-50000.cptk.data-00000-of-00001 model-50001.cptk.meta\r\n",
      "model-50000.cptk.index               raw_graph_def.pb\r\n",
      "model-50000.cptk.meta\r\n"
     ]
    }
   ],
   "source": [
    "!ls ./models/ppo/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 참고자료\n",
    "* [1] 유니티의 ML-agents 공식 github - https://github.com/Unity-Technologies/ml-agents\n",
    "* [2] wonseok Jung's Handmade RL -2 - https://wonseokjung.github.io/unity/update/unity-2/\n",
    "* [3] Setting up a Python Environment with Unity ML-Agents and TensorFlow for macOS - https://medium.com/@indiecontessa/setting-up-a-python-environment-with-tensorflow-on-macos-for-training-unity-ml-agents-faf19d71201"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
